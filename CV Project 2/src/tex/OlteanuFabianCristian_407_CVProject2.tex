\documentclass{article}
\usepackage[a4paper, portrait, margin=1.1811in]{geometry}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{helvet}
\usepackage{etoolbox}
\usepackage{graphicx}
\usepackage{titlesec}
\usepackage{caption}
\usepackage{booktabs}
\usepackage{xcolor} 
\usepackage[colorlinks, citecolor=cyan]{hyperref}
\usepackage{caption}
\captionsetup[figure]{name=Figure}
\graphicspath{ {./images/} }
\usepackage{scrextend}
\usepackage{fancyhdr}
\usepackage{graphicx}
\newcounter{lemma}
\newtheorem{lemma}{Lemma}
\newcounter{theorem}
\newtheorem{theorem}{Theorem}

\fancypagestyle{plain}{
	\fancyhf{}
	\renewcommand{\headrulewidth}{0pt}
	\renewcommand{\familydefault}{\sfdefault}
}

%\pagestyle{plain}
\makeatletter
\patchcmd{\@maketitle}{\LARGE \@title}{\fontsize{16}{19.2}\selectfont\@title}{}{}
\makeatother

\usepackage{authblk}
\renewcommand\Authfont{\fontsize{10}{10.8}\selectfont}
\renewcommand\Affilfont{\fontsize{10}{10.8}\selectfont}
\renewcommand*{\Authsep}{, }
\renewcommand*{\Authand}{, }
\renewcommand*{\Authands}{, }
\setlength{\affilsep}{2em}  
\newsavebox\affbox
\author{\textbf{Olteanu Fabian Cristian}}
\affil{FMI, AI Master, Year 1
}

\titlespacing\section{0pt}{12pt plus 4pt minus 2pt}{0pt plus 2pt minus 2pt}
\titlespacing\subsection{12pt}{12pt plus 4pt minus 2pt}{0pt plus 2pt minus 2pt}
\titlespacing\subsubsection{12pt}{12pt plus 4pt minus 2pt}{0pt plus 2pt minus 2pt}


\titleformat{\section}{\normalfont\fontsize{10}{15}\bfseries}{\thesection.}{1em}{}
\titleformat{\subsection}{\normalfont\fontsize{10}{15}\bfseries}{\thesubsection.}{1em}{}
\titleformat{\subsubsection}{\normalfont\fontsize{10}{15}\bfseries}{\thesubsubsection.}{1em}{}

\titleformat{\author}{\normalfont\fontsize{10}{15}\bfseries}{\thesection}{1em}{}

\title{\textbf{\huge Computer Vision Project 2}}
\date{}    

\begin{document}

\pagestyle{headings}	
\newpage
\setcounter{page}{1}
\renewcommand{\thepage}{\arabic{page}}


	
\captionsetup[figure]{labelfont={bf},labelformat={default},labelsep=period,name={Figure }}	\captionsetup[table]{labelfont={bf},labelformat={default},labelsep=period,name={Table }}
\setlength{\parskip}{0.5em}
	
\maketitle
	
\noindent\rule{15cm}{0.4pt}

\section{Introduction}
At the core of both approaches which were taken to tackle tasks one and two lies the a pre-trained version (using the MS COCO dataset \cite{MSCOCO}) of the deep neural network YOLOv5s\cite{yolov5} by Ultralytics. The "s" variant of the model was chosen for its lightweightness, ease of use with the OpenCV Python library and good object detection accuracy. It also features the ability to detect and differentiate between 80 classes of objects, although it was restricted to only five of them (bicycles, cars, motorbikes, busses and trucks). 

YOLOv5s takes as input images in the RGB format with pixel values between zero and one and of size 640x640. Based on this input, the model generates prediction as a data structure comprised of a 25200x85 list of lists, each row representing the x,y,w,h coordinates of the detected bounding box rectangle, the confidence level of the detection and the scores of each of the 80 classes.

\section{Task 1}
The steps taken to achieve a 97\% accuracy on the training dataset are summerized as follows:

\begin{enumerate}
	\item The coordinates of the four corners of each polygon representing the lanes were hand-picked and stored in nine lists. The Python library "shapely"\cite{shapely} was also used in order to compute the areas of intersection between the lanes and the detected bounding boxes, which will be described in the next steps.
	\item The YOLOv5s pre-trained model is loaded into OpenCV (using the compatible .onnx converted format) and is used in order to obtain the bounding box of the five specified types of vehicles (example in figure 1). This is done by unwrapping the predictions generated by the model of an image using some constants taking the roles of tresholds. The values of these thresholds are as follows: 
		\begin{itemize}
			\item Minimum confidence threshold: 0.4,
			\item Non-maximum suppression threshold (to avoid duplicate, overlapping bounding boxes): 0.45,
			\item Score threshold: 0.25.
		\end{itemize}
The values were obtained through trial and error (lowering or increasing the confidence threshold was found to cause the most differences in accuracy).
	\item In order to determine which lanes are occupied in an image, an algorithm involving the maximum intersection of the bounding box and lane polygons had to be devised. For an individual image, the algorithm goes through each of the bounding boxes detected at the prior step. For each lane, it tries to find the maximum overlap between the bounding box and the lane, that is the maximum area of the intersection between these two polygons. For lanes four through 6, because of the perspective, it is better to compute this overlap based on the lower half of the vehicle bounding box and the lane. Finally, after the maximum area has been computed, a threshold of 500 is imposed so that situations such as those when a vehicle's bounding box that is outside of the lanes slightly intersects them are avoided. The lane corresponding to the maximum overlap is marked as occupied.
\end{enumerate}

\begin{figure}[hbt!]
	\centering
	\includegraphics[scale=0.25]{fig1.jpg}
	\caption{Detection of vehicles and highlighting of lanes in 01\_1.jpg}
\end{figure}

\section{Task 2}
The solution implemented for Task 2 also fully takes advantage of YOLOv5s for detection. The tracking algorithm works as follows:
\begin{enumerate}
	\item Likewise to task 1, the predictions for an image are unwrapped in order to obtain the bounding boxes of the detected vehicles in the image (in this case the image is a frame of the 60fps video).
	\item The initial tracked bounding box is extracted from the query text file and assigned to a tracked bounding box list.
	\item For each frame in the video, the current tracked object bounding box (located in the tracked object bounding box list at the previous position) has some padding added (50 pixels - the value was found through trial and error) and is then extracted from the initial image as a separate image. The model is used to detect the position of the new bounding box. This is done achieved through detections of the bounding box in the restricted area, calculating the maximum intersection over union for each detection with the tracked bounding box at the last step. If there are no detections at the current frame, the tracked bounding box from the last step is appended to the array one more time.
\end{enumerate}

\newpage

\begin{thebibliography}{1.7} 
	\bibitem[1]{MSCOCO} \color{cyan}\url{https://cocodataset.org/#home} \color{black}
	\bibitem[2]{yolov5} \color{cyan}\url{https://github.com/ultralytics/yolov5/wiki} \color{black}
	\bibitem[3]{shapely} \color{cyan}\url{https://pypi.org/project/shapely/} \color{black}
	
	
\end{thebibliography}


\end{document}